---
title: "Forecasting Walmart Weekly Sales"
subtitle: "Forecasting for Supply Chain, End-of-Semester Project"
author: "Behrouz Delfanian, Hesam Korki"
date: "28 January 2026"
format: 
  html:
    code-fold: true
    code-overflow: scroll
    code-line-numbers: true
    code-tools: true
    df_print: paged
    toc: true
    toc-location: left
    toc-depth: 3
    smooth-scroll: true
    embed-resources: true
    theme: cosmo
---

# Setup and Environment

## Required Libraries

```{r librairies-to-install}
#| cache: true
#| output: false
cran_repo <- "https://ftp.fau.de/cran"
list_Rpack2use <-
  c("rmarkdown",
    "conflicted",
    "devtools",
    "knitr",
    "quarto",
    "bookdown",
    "DT",
    "Hmisc",
    "tidyverse",
    "magrittr",
    "lubridate",
    "hms",
    "glue",
    "skimr",
    "lobstr",
    "janitor",
    "zeallot",
    "stringi",
    "lemon",
    "crayon",
    "jsonlite",
    "TSA",
    "tsibble",
    "fable",
    "feasts",
    "imputeTS",
    "readxl",
    "slider",
    "reticulate",
    "aws.s3",
    "paws",
    "RAthena",
    "botor",
    "RPostgres",
    "DBI",
    "RJDBC",
    "dbplyr",
    "arrow",
    "geosphere",
    "viridis")

list_installed_packages <- names(installed.packages()[,2])
for (Rpack in list_Rpack2use) {
  if (! Rpack %in% list_installed_packages) {
    print(paste("Installing", Rpack))
    install.packages(Rpack, repos = cran_repo, )
  } else {
    print(paste(Rpack, "already installed."))
  }
}

```

## Load Libraries & Resolve Conflicts

```{r librairies-to-load}
#| cache: false

list_Rpack2load_not <-
  c("conflicted",
    "devtools",
    "paws",
    "RAthena",
    "botor",
    "arrow",
    "RPostgres",
    "DBI",
    "RJDBC",
    "dbplyr",
    "aws.s3",
    "arrow",
    "reticulate",
    "slider",
    "stringi",
    "geosphere",
    "blah")
list_Rpack2load <- setdiff(list_Rpack2use, list_Rpack2load_not)
list_Rpack2load

```

```{r loading-libraries}
#| cache: false
#| output: false
  
for (Rpack in list_Rpack2load) {
  library(Rpack, character.only = TRUE)
}

conflicted::conflicts_prefer(pillar::dim_desc)
conflicted::conflicts_prefer(magrittr::extract)
conflicted::conflicts_prefer(dplyr::filter)
conflicted::conflicts_prefer(jsonlite::flatten)
conflicted::conflicts_prefer(hms::hms)
conflicted::conflicts_prefer(dbplyr::ident)
conflicted::conflicts_prefer(lubridate::interval)
conflicted::conflicts_prefer(dplyr::lag)
conflicted::conflicts_prefer(readxl::read_xlsx)
conflicted::conflicts_prefer(magrittr::set_names)
conflicted::conflicts_prefer(dbplyr::sql)
conflicted::conflicts_prefer(Hmisc::src)
conflicted::conflicts_prefer(dplyr::summarize)
conflicted::conflicts_prefer(magrittr::is_in)

```

# Data Loading & Preparation

## Directory & Data Paths

```{r data_paths}

dir_data <- "./data/Walmart/"

if (!dir.exists(dir_data)) {
  dir.create(dir_data)
}

path_train    <- file.path(dir_data, "train.csv")
path_test     <- file.path(dir_data, "test.csv")
path_features <- file.path(dir_data, "features.csv")
path_stores   <- file.path(dir_data, "stores.csv")

```

## Read Raw Data

```{r read_raw_data}
train_raw <- read_csv(path_train, show_col_types = FALSE)
test_raw  <- read_csv(path_test,  show_col_types = FALSE)
features  <- read_csv(path_features, show_col_types = FALSE)
stores    <- read_csv(path_stores, show_col_types = FALSE)
```

## Merge Data

```{r data_merging}
# Convert dates & create key variables
train <- train_raw |>
  mutate(
    Date = ymd(Date),
    Store_Dept = glue("{Store}-{Dept}"),
    IsHoliday = as.logical(IsHoliday)
  ) |>
  left_join(features |> mutate(Date = ymd(Date)), by = c("Store", "Date", "IsHoliday")) |>
  left_join(stores, by = "Store") |>
  as_tsibble(key = Store_Dept, index = Date)
```

## Check Missing Values

```{r check_missing_values}
na_pct <- colMeans(is.na(train)) * 100
na_pct <- na_pct[na_pct > 0]

data.frame(
  Column = names(na_pct),
  "NA_Percent" = round(na_pct, 2)
) |>
  arrange(desc("NA_Percent"))
```

## Clean Data

**Markdown1** to **Markdown5** columns represent **markdown/promotional discounts** (temporary price reductions) applied in each store on a given week. We replace `NA` with `0` (meaning no markdown/promotion that week). We also remove observations with zero or negative values for `Weekly_Sales`.

```{r clean_data}
train_clean <- train |>
  mutate(
    across(starts_with("MarkDown"), ~ replace_na(., 0))
  )

# Detect problematic sales values
sales_issues <- train_clean |>
  summarise(
    Total_rows          = n(),
    Negative_sales      = sum(Weekly_Sales < 0, na.rm = TRUE),
    Zero_sales          = sum(Weekly_Sales == 0, na.rm = TRUE),
    Negative_or_zero    = sum(Weekly_Sales <= 0, na.rm = TRUE),
    Pct_negative_or_zero = round(mean(Weekly_Sales <= 0, na.rm = TRUE) * 100, 2)
  )

print("Sales quality summary:")
print(sales_issues)

# Remove negative and zero sales (most common and reasonable approach for this project)
train_clean <- train_clean |>
  filter(Weekly_Sales > 0)

# Final check
cat("Rows after removal:", nrow(train_clean), "\n")
cat("Percentage of rows removed:", 
    round((nrow(train) - nrow(train_clean)) / nrow(train) * 100, 2), "%\n")
```

## Train/Test Split

```{r data_split}
all_dates   <- sort(unique(train_clean$Date))
cutoff_date <- all_dates[length(all_dates) - 4]

train_set <- train_clean |> filter(Date <= cutoff_date)
eval_set  <- train_clean |> filter(Date >  cutoff_date)

# Quick verification
cat("Training weeks: ", n_distinct(train_set$Date), "\n")
cat("Evaluation weeks: ", n_distinct(eval_set$Date), "\n")
```

## Different Aggregation Levels

```{r aggregation_levels}
## Create different aggregation levels for Part I comparison

# 1. Item level (Store × Dept) - direct forecasting
item_ts <- train_set |>
  mutate(Week = yearweek(Date)) |>
  as_tsibble(key = c(Store, Dept), index = Week)

# 2. Store level - aggregate then disaggregate
store_ts <- train_set |>
  group_by(Store, Week = yearweek(Date)) |>
  summarise(
    Total_Sales = sum(Weekly_Sales, na.rm = TRUE),
    IsHoliday   = any(IsHoliday),
    .groups     = "drop"
  ) |>
  as_tsibble(key = Store, index = Week)

# 3. Department level
dept_ts <- train_set |>
  group_by(Dept, Week = yearweek(Date)) |>
  summarise(
    Total_Sales = sum(Weekly_Sales, na.rm = TRUE),
    IsHoliday   = any(IsHoliday),
    .groups     = "drop"
  ) |>
  as_tsibble(key = Dept, index = Week)
```

## Quick Overview

```{r quick_overview}
# Number of series
n_series <- n_distinct(train$Store_Dept)
glue("Number of Store-Department time series: {n_series}")

# Time range
range(train$Date)
range(train_set$Date)
range(eval_set$Date)
```

# Time Series Exploratory Analysis

## Seasonality Comparison

To compare the three time series in a normalized way, we apply **Divide-by-Mean** normalization technique.

```{r visual_comparison}
# ── Normalized versions for visual comparison ────────────────────────────────

item_10_10_norm <- item_ts |>
  filter(Store == 10, Dept == 10) |>
  as_tibble() |>
  mutate(
    Sales_norm = Weekly_Sales / mean(Weekly_Sales, na.rm = TRUE),
    Series     = "Store 10 - Dept 10 (item)"
  ) |>
  select(Week, Sales_norm, Series)

store_10_norm <- store_ts |>
  filter(Store == 10) |>
  as_tibble() |>
  mutate(
    Sales_norm = Total_Sales / mean(Total_Sales, na.rm = TRUE),
    Series     = "Store 10 - Total (sum over depts)"
  ) |>
  select(Week, Sales_norm, Series)

dept_10_norm <- dept_ts |>
  filter(Dept == 10) |>
  as_tibble() |>
  mutate(
    Sales_norm = Total_Sales / mean(Total_Sales, na.rm = TRUE),
    Series     = "Dept 10 - Total (sum over stores)"
  ) |>
  select(Week, Sales_norm, Series)

combined_norm <- bind_rows(
  item_10_10_norm,
  store_10_norm,
  dept_10_norm
)

# ── Plot: normalized series (overlay) ────────────────────────────────────────

ggplot(combined_norm, aes(x = Week, y = Sales_norm, color = Series)) +
  geom_line(linewidth = 0.7) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "grey50") +
  scale_color_manual(values = c(
    "Store 10 - Dept 10 (item)"          = "#1f77b4",
    "Store 10 - Total (sum over depts)"  = "#ff7f0e",
    "Dept 10 - Total (sum over stores)"  = "#2ca02c"
  )) +
  labs(
    title    = "Normalized Weekly Sales (divided by own mean)",
    subtitle = "Comparison of shapes: item vs store aggregate vs department aggregate",
    y        = "Normalized sales (mean = 1)",
    x        = "Week",
    color    = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "bottom",
    plot.title      = element_text(face = "bold")
  )

# Faceted version (easier to compare individually)
ggplot(combined_norm, aes(x = Week, y = Sales_norm)) +
  geom_line(color = "#1f77b4", linewidth = 0.7) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "grey50") +
  facet_wrap(~ Series, ncol = 1, scales = "free_y") +
  labs(
    title    = "Normalized Weekly Sales by Series (mean = 1)",
    y        = "Normalized sales",
    x        = NULL
  ) +
  theme_minimal() +
  theme(strip.text = element_text(face = "bold"))
```

We observe much clearer weekly seasonality at the store-total level than at the individual item level.

- **Seasonality strength**: usually strongest at aggregate levels (store total).

- **Noise level**: highest at item level, lowest at aggregates.

- **Holiday effects**: often more visible/clear in aggregates

This suggests that aggregating reduces noise and makes seasonal patterns more detectable → likely benefit for exponential smoothing methods.


## Frequency analysis (periodogram / FFT)


## STL decomposition (trend / seasonal / remainder)


## Additive vs Multiplicative seasonality check


## Stationarity assessment


## Selection of aggregation levels (store / department groups)



# Part I – Forecasting Comparison

We compare two approaches on the last 4 weeks:

1. Direct forecasting at item level (Store 10 – Dept 10)  

2. Forecasting at store level (Store 10 total) → proportional de-aggregation to item level

Both methods use self-implemented Triple Exponential Smoothing (Holt-Winters).

## Self-implemented TSE functions

```{r tse_functions}
# ── Level-only: Simple Exponential Smoothing (SES) ───────────────────────
ses_level <- function(y, alpha, l0 = NULL) {
  n <- length(y)
  l <- numeric(n)
  l[1] <- if (is.null(l0)) y[1] else l0
  for (t in 2:n) {
    l[t] <- alpha * y[t] + (1 - alpha) * l[t-1]
  }
  l
}

ses_forecast1 <- function(l, alpha) {
  l[length(l)]   # horizon 1 forecast = last smoothed level
}

# ── Level + Trend: Double Exponential Smoothing (Holt) ───────────────────
des <- function(y, alpha, beta, l0 = NULL, b0 = NULL, damped = FALSE, phi = 1) {
  n <- length(y)
  l <- b <- numeric(n)
  
  # Initialization (simple heuristic from course)
  l[1] <- if (is.null(l0)) y[1] else l0
  b[1] <- if (is.null(b0)) y[2] - y[1] else b0
  
  for (t in 2:n) {
    l[t] <- alpha * y[t] + (1 - alpha) * (l[t-1] + phi * b[t-1])
    b[t] <- beta  * (l[t] - l[t-1]) + (1 - beta) * phi * b[t-1]
  }
  
  list(level = l, trend = b)
}

des_forecast1 <- function(fit, phi = 1) {
  last_l <- tail(fit$level, 1)
  last_b <- tail(fit$trend, 1)
  last_l + phi * last_b
}

# ── Full TES (Holt-Winters) - additive version ───────────────────────────
tes_additive <- function(y, alpha, beta, gamma, period = 52, 
                         l0 = NULL, b0 = NULL, s0 = NULL, damped = FALSE, phi = 1) {
  n <- length(y)
  if (n < period + 1) stop("Series too short for seasonal model (need > period + 1 points)")
  l <- b <- s <- numeric(n)
  
  # Initialization (course style: average first period for seasonal)
  if (is.null(l0)) l0 <- mean(y[1:period], na.rm = TRUE)
  if (is.null(b0)) b0 <- (mean(y[(period+1):(2*period)]) - mean(y[1:period])) / period
  if (is.null(s0)) s0 <- y[1:period] - l0
  
  l[1] <- l0
  b[1] <- b0
  s[1:period] <- s0
  
  for (t in (period + 1):n) {
    l[t] <- alpha * (y[t] - s[t-period]) + (1 - alpha) * (l[t-1] + phi * b[t-1])
    b[t] <- beta  * (l[t] - l[t-1]) + (1 - beta) * phi * b[t-1]
    s[t] <- gamma * (y[t] - l[t]) + (1 - gamma) * s[t-period]
  }
  
  list(level = l, trend = b, season = s)
}

tes_add_forecast1 <- function(fit, period = 52, phi = 1) {
  last_l <- tail(fit$level, 1)
  last_b <- tail(fit$trend, 1)
  m <- length(fit$season)
  next_season_idx <- ((m - 1) %% period) + 1
  next_s <- fit$season[next_season_idx]
  last_l + phi * last_b + next_s
}

# ── TES Multiplicative version ───────────────────────────────────────────

tes_multiplicative <- function(y, alpha, beta, gamma, period = 52,
                               l0 = NULL, b0 = NULL, s0 = NULL,
                               damped = FALSE, phi = 1) {
  n <- length(y)
  if (n < period + 1) stop("Series too short for seasonal model (need > period + 1 points)")
  l <- b <- s <- numeric(n)
  
  # Initialization (common heuristic)
  if (is.null(l0)) l0 <- mean(y[1:period], na.rm = TRUE)
  if (any(s0 <= 0)) {
  warning("Zero or negative seasonal components detected – replacing with small value")
  s0[s0 <= 0] <- 1e-6
  }
  if (is.null(b0)) b0 <- (mean(y[(period+1):(2*period)]) - mean(y[1:period])) / period
  if (is.null(s0)) s0 <- y[1:period] / l0                     # ← key difference
  
  l[1] <- l0
  b[1] <- b0
  s[1:period] <- s0
  
  for (t in (period + 1):n) {
    l[t] <- alpha * (y[t] / s[t - period]) + (1 - alpha) * (l[t-1] + phi * b[t-1])
    b[t] <- beta  * (l[t] - l[t-1])            + (1 - beta)  * phi * b[t-1]
    s[t] <- gamma * (y[t] / l[t])              + (1 - gamma) * s[t - period]
  }
  
  list(level = l, trend = b, season = s)
}

# One-step forecast
tes_multi_forecast1 <- function(fit, period = 52, phi = 1) {
  last_l <- tail(fit$level,  1)
  last_b <- tail(fit$trend,  1)
  m <- length(fit$season)
  next_season_idx <- ((m - 1) %% period) + 1
  next_s <- fit$season[next_season_idx]
  
  (last_l + phi * last_b) * next_s
}

# SSE objective for TES additive
sse_tes_add <- function(params, y, period = 52, damped = FALSE, phi = 1) {
  alpha <- params[1]; beta <- params[2]; gamma <- params[3]
  if (any(c(alpha, beta, gamma) <= 0) || any(c(alpha, beta, gamma) >= 1)) return(Inf)

  fit <- tes_additive(y, alpha, beta, gamma, period, damped = damped, phi = phi)
  errors <- y - (fit$level + fit$trend * phi + fit$season)   # in-sample one-step errors
  sum(errors^2, na.rm = TRUE)
}

sse_tes_multi <- function(params, y, period = 52, phi = 1) {
  alpha <- params[1]; beta <- params[2]; gamma <- params[3]
  if (any(c(alpha, beta, gamma) <= 0) || any(c(alpha, beta, gamma) >= 1)) return(Inf)
  
  fit <- tes_multiplicative(y, alpha, beta, gamma, period, phi = phi)
  
  # One-step fitted values
  fitted <- numeric(length(y))
  for (t in 1:length(y)) {
    s_idx <- ((t - 1) %% period) + 1
    fitted[t] <- (fit$level[t] + phi * fit$trend[t]) * fit$season[s_idx]
  }
  
  errors <- y - fitted
  sum(errors^2, na.rm = TRUE)
}

# Grid search example (coarse)
grid_search_tes_add <- function(y, period = 52) {
  cat("Starting grid search (additive)...\n")
  
  alphas <- seq(0.1, 0.9, 0.1)
  betas  <- seq(0.01, 0.5, 0.05)
  gammas <- seq(0.1, 0.8, 0.1)
  
  best_sse <- Inf
  best_par <- c(0.5, 0.1, 0.3)
  
  for (a in alphas) for (b in betas) for (g in gammas) {
    sse <- sse_tes_add(c(a,b,g), y, period)
    if (sse < best_sse) {
      best_sse <- sse
      best_par <- c(a,b,g)
    }
  }
  list(alpha = best_par[1], beta = best_par[2], gamma = best_par[3], sse = best_sse)
}

grid_search_tes_multi <- function(y, period = 52) {
  cat("Starting grid search (multiplicative)...\n")
  
  alphas <- seq(0.1, 0.9, 0.1)
  betas  <- seq(0.01, 0.5, 0.05)
  gammas <- seq(0.1, 0.8, 0.1)
  
  best_sse <- Inf
  best_par <- c(0.5, 0.1, 0.3)
  
  for (a in alphas) for (b in betas) for (g in gammas) {
    sse <- sse_tes_multi(c(a,b,g), y, period)
    if (sse < best_sse) {
      best_sse <- sse
      best_par <- c(a,b,g)
    }
  }
  list(alpha = best_par[1], beta = best_par[2], gamma = best_par[3], sse = best_sse)
}

# Rolling horizon-1 forecasts – generic for any fitted TSE model
rolling_h1_forecasts <- function(data, y_var = "y", period = 52,
                                 params, model = "multi", phi = 1) {
  
  eval_weeks <- tail(unique(data$Week), 4)
  results <- tibble(Week = eval_weeks, Actual = NA_real_, Forecast = NA_real_)
  
  for (i in seq_along(eval_weeks)) {
    train <- data |> filter(Week < eval_weeks[i])
    y_train <- train[[y_var]]
    
    
    if (model == "add") {
      fit <- tes_additive(y_train, params$alpha, params$beta, params$gamma,
                          period = period, phi = phi)
      f1 <- tes_add_forecast1(fit, period = period, phi = phi)
    } else {
      fit <- tes_multiplicative(y_train, params$alpha, params$beta, params$gamma,
                                period = period, phi = phi)
      f1 <- tes_multi_forecast1(fit, period = period, phi = phi)
    }
    
    actual <- data |> filter(Week == eval_weeks[i]) |> pull({{y_var}})
    
    results$Actual[i]   <- actual
    results$Forecast[i] <- f1
  }
  
  results |> mutate(Error = Actual - Forecast)
}
```


## Method 1: Direct forecasting at Store-Dept level

```{r item_series}
# ── Prepare the series for Store 10 – Dept 10 ────────────────────────────────
item_series <- item_ts |>
  filter(Store == 10, Dept == 10) |>
  mutate(y = Weekly_Sales) |>
  select(Week, y, IsHoliday) |>
  as_tibble()   # easier for fitting

# Quick check
glimpse(item_series)
```

### Parameter Optimization

Optimize parameters using multiplicative model.

```{r parameter_optimization_multi}
params_item_multi <- grid_search_tes_multi(item_series$y, period = 52)

# Show results
cat("Best parameters (multiplicative) for Store 10 – Dept 10:\n")
print(params_item_multi)
```
- **α = 0.9** (level smoothing parameter)  
Very high value → the model puts **strong weight on the most recent observation** when updating the level (smoothed baseline).  
→ The level adapts **very quickly** to recent changes in the series.  

**β = 0.01** (trend smoothing parameter)  
Extremely low value → the trend component is updated **very slowly** / almost not at all.  
→ The model essentially treats the trend as **nearly constant** over time.  
→ This is common when there is **little or no clear long-term trend** in the data, or when trend changes are very gradual.

**γ = 0.1** (seasonal smoothing parameter)  
Moderate-to-low value → the seasonal pattern is updated **slowly**.  
→ The model trusts the **historical seasonal pattern quite strongly** and only makes small adjustments when new data arrives.  
→ Suitable when seasonality is **stable over time** (which is often the case at aggregate levels in retail data).

**Overall interpretation in context**  
- The model is behaving almost like **simple exponential smoothing with a fixed seasonal pattern** (because β is extremely small and γ is low).  
- The very high α (0.9) indicates that the series is **noisy or has fast level shifts**, so the model reacts strongly to recent values.  


In short:

> The optimized parameters (α = 0.9, β = 0.01, γ = 0.1) indicate that the model relies heavily on recent observations to update the level (high α), while treating trend as almost constant (very low β) and preserving a stable seasonal pattern (low γ). This is consistent with the noisy and irregular behavior observed at the individual item level.

Also additive model for comparison:

```{r parameter_optimization_add}
params_item_add <- grid_search_tes_add(item_series$y, period = 52)
cat("\nBest parameters (additive) for Store 10 – Dept 10:\n")
print(params_item_add)
```


### Rolling Horizon-1 Forecasts

```{r horizon_1_forecast}
# Rolling forecasts – multiplicative version
fc_direct_multi <- rolling_h1_forecasts(
  data   = item_series,
  y_var  = "y",
  period = 52,
  params = params_item_multi,
  model  = "multi",
  phi    = 1
)

# Additive version
fc_direct_add <- rolling_h1_forecasts(
  data   = item_series,
  y_var  = "y",
  period = 52,
  params = params_item_add,
  model  = "add",
  phi    = 1
)

# Combine for comparison
fc_compare <- bind_rows(
  fc_direct_multi |> mutate(Model = "Multiplicative"),
  fc_direct_add   |> mutate(Model = "Additive")
)
```

### Visual Check

```{r visual_check}
# Plot comparison
ggplot(fc_compare, aes(x = Week)) +
  geom_line(aes(y = Actual, color = "Actual"), linewidth = 0.9) +
  geom_point(aes(y = Forecast, color = Model), size = 2.5) +
  geom_line(aes(y = Forecast, color = Model), linetype = "dashed") +
  scale_color_manual(values = c("Actual" = "black", "Multiplicative" = "red", "Additive" = "blue")) +
  labs(
    title = "Direct forecasting – Additive vs Multiplicative",
    subtitle = "Store 10 – Dept 10 | Last 4 weeks",
    y = "Weekly Sales"
  ) +
  theme_minimal()
```

### Accuracy Evaluation

We evaluate forecast accuracy using the following four measures:

- **MAE** (Mean Absolute Error)  
  Average absolute difference between forecast and actual.  
  Easy to interpret in the original units ($), robust to outliers compared to squared errors.

- **WAPE** (Weighted Absolute Percentage Error) = Σ |error| / Σ actual × 100  
  Scale-independent version of absolute error — expresses total error relative to total demand.  
  Preferred in retail forecasting when aggregating over items or periods (avoids division by small/zero actuals).

- **MAPE** (Mean Absolute Percentage Error) = mean(|error / actual| × 100)  
  Classic percentage error measure — shows average relative error per observation.  
  Intuitive, but sensitive when actual values are close to zero (which we mitigated by removing zero sales earlier).

- **RMSE** (Root Mean Squared Error)  
  Square root of average squared errors — penalizes large errors more heavily.  
  Useful when large deviations are particularly costly (common in inventory/supply chain contexts).

These metrics are standard in forecasting literature and were covered/discussed in the course materials (especially in sessions on model evaluation and confidence intervals).  

They provide complementary views:  

- MAE & RMSE → absolute scale & outlier sensitivity  

- MAPE & WAPE → relative performance & scale-invariance

We report all four to give a complete picture, with **WAPE** receiving particular attention because it is widely used in retail demand forecasting (including the M5 competition from which this dataset originates).

```{r accuracy_comparison}
# Accuracy comparison table
accuracy_compare <- fc_compare |>
  group_by(Model) |>
  summarise(
    MAE  = mean(abs(Error), na.rm = TRUE),
    WAPE = sum(abs(Error), na.rm = TRUE) / sum(Actual, na.rm = TRUE) * 100,
    MAPE = mean(abs(Error / Actual), na.rm = TRUE) * 100,
    RMSE = sqrt(mean(Error^2, na.rm = TRUE))
  ) |>
  mutate(across(where(is.numeric), ~round(., 2)))

knitr::kable(accuracy_compare, caption = "Method 1 – Additive vs Multiplicative")
```


## Method 2: Aggregate → Forecast → Proportional de-aggregation

Preparing aggregate series (Store 10 total sales).

```{r aggregated_series}
store_series <- store_ts |>
  filter(Store == 10) |>
  mutate(y = Total_Sales) |>
  select(Week, y, IsHoliday) |>
  as_tibble()

cat("Store 10 total series length:", nrow(store_series), "weeks\n")
```

## Parameter Optimization

Multiplicative (stronger seasonality at aggregate level):

```{r agg_para_opti_multi}
params_store_multi <- grid_search_tes_multi(store_series$y, period = 52)

cat("Best parameters (multiplicative) for Store 10 total:\n")
print(params_store_multi)
```
Additive for comparison:

```{r agg_para_opti_add}
params_store_add <- grid_search_tes_add(store_series$y, period = 52)
cat("\nBest parameters (additive):\n")
print(params_store_add)
```

## Rolling Horizon-1 Forecast

Using multiplicative model:

```{r agg_horizon_1_forecast_multi}
fc_aggregate_multi <- rolling_h1_forecasts(
  data   = store_series,
  y_var  = "y",
  period = 52,
  params = params_store_multi,
  model  = "multi",
  phi    = 1
)
```


## Proportional De-aggregation

We compare three de-aggregation strategies:

1. **Static**: Historical mean proportion (simple but ignores seasonal variation)
2. **Rolling**: 4-week rolling average proportion (adapts to recent trends)
3. **Seasonal**: Same-week-of-year average proportion (captures weekly seasonality)

```{r de_aggregation}
# Calculate proportions over time
prop_data <- item_series |>
  left_join(store_series |> select(Week, Total = y), by = "Week") |>
  mutate(prop = y / Total)

# 1. STATIC: Historical average proportion (original method)
prop_static <- mean(prop_data$prop, na.rm = TRUE)
cat("Method 1 - STATIC proportion:", round(prop_static, 4), "\n")

# 2. SEASONAL: Same-week-of-year average proportion
# Extract week-of-year from yearweek
prop_data <- prop_data |>
  mutate(week_of_year = isoweek(as.Date(Week)))

seasonal_props <- prop_data |>
  group_by(week_of_year) |>
  summarise(seasonal_prop = mean(prop, na.rm = TRUE), .groups = "drop")

cat("Method 2 - SEASONAL proportions calculated (", nrow(seasonal_props), " unique weeks)\n")

# 3. ROLLING: 4-week rolling average proportion
prop_data <- prop_data |>
  arrange(Week) |>
  mutate(rolling_prop = slider::slide_dbl(prop, mean, .before = 3, .complete = TRUE))

cat("Method 3 - ROLLING (4-week) last value:", 
    round(tail(na.omit(prop_data$rolling_prop), 1), 4), "\n\n")
```

### Rolling De-aggregation with All Methods

```{r de_aggregation_comparison}
# Perform rolling de-aggregation with all three methods
eval_weeks <- tail(unique(store_series$Week), 4)

deagg_results <- tibble(

  Week = eval_weeks,
  Actual = numeric(4),
  Forecast_agg = numeric(4),
  De_agg_static = numeric(4),
  De_agg_seasonal = numeric(4),
  De_agg_rolling = numeric(4)
)

for (i in seq_along(eval_weeks)) {
  eval_week <- eval_weeks[i]
  
  # Get aggregate forecast for this week
  agg_forecast <- fc_aggregate_multi |> 
    filter(Week == eval_week) |> 
    pull(Forecast)
  
  # Get actual item-level sales
  actual_item <- item_series |> 
    filter(Week == eval_week) |> 
    pull(y)
  
  # Get proportions for de-aggregation (using data before forecast week)
  train_prop_data <- prop_data |> filter(Week < eval_week)
  
  # Static proportion
  p_static <- prop_static
  
  # Seasonal proportion (for this week-of-year)
  forecast_woy <- isoweek(as.Date(eval_week))
  p_seasonal <- seasonal_props |> 
    filter(week_of_year == forecast_woy) |> 
    pull(seasonal_prop)
  if (length(p_seasonal) == 0) p_seasonal <- prop_static
  
  # Rolling proportion (last 4-week average before forecast)
  p_rolling <- tail(na.omit(train_prop_data$rolling_prop), 1)
  if (length(p_rolling) == 0) p_rolling <- prop_static
  
  # Store results
  deagg_results$Actual[i] <- actual_item
  deagg_results$Forecast_agg[i] <- agg_forecast
  deagg_results$De_agg_static[i] <- agg_forecast * p_static
  deagg_results$De_agg_seasonal[i] <- agg_forecast * p_seasonal
  deagg_results$De_agg_rolling[i] <- agg_forecast * p_rolling
}

# Calculate errors
deagg_results <- deagg_results |>
  mutate(
    Error_static = Actual - De_agg_static,
    Error_seasonal = Actual - De_agg_seasonal,
    Error_rolling = Actual - De_agg_rolling
  )

# Display results
deagg_results |>
  select(Week, Actual, De_agg_static, De_agg_seasonal, De_agg_rolling) |>
  knitr::kable(caption = "De-aggregation Comparison: Forecasts by Method", digits = 2)
```

### De-aggregation Method Accuracy

```{r deagg_accuracy}
# Calculate accuracy metrics for each de-aggregation method
deagg_accuracy <- bind_rows(
  deagg_results |>
    summarise(
      Method = "Static (mean prop)",
      MAE = mean(abs(Error_static)),
      WAPE = sum(abs(Error_static)) / sum(Actual) * 100,
      MAPE = mean(abs(Error_static / Actual)) * 100,
      RMSE = sqrt(mean(Error_static^2))
    ),
  deagg_results |>
    summarise(
      Method = "Seasonal (week-of-year)",
      MAE = mean(abs(Error_seasonal)),
      WAPE = sum(abs(Error_seasonal)) / sum(Actual) * 100,
      MAPE = mean(abs(Error_seasonal / Actual)) * 100,
      RMSE = sqrt(mean(Error_seasonal^2))
    ),
  deagg_results |>
    summarise(
      Method = "Rolling (4-week)",
      MAE = mean(abs(Error_rolling)),
      WAPE = sum(abs(Error_rolling)) / sum(Actual) * 100,
      MAPE = mean(abs(Error_rolling / Actual)) * 100,
      RMSE = sqrt(mean(Error_rolling^2))
    )
) |>
  mutate(across(where(is.numeric), ~round(., 2)))

knitr::kable(deagg_accuracy, 
             caption = "De-aggregation Method Comparison – Accuracy Metrics")
```

```{r best_deagg_method}
# Select best method for final comparison
best_method <- deagg_accuracy |> 
  slice_min(WAPE, n = 1) |> 
  pull(Method)

cat("Best de-aggregation method:", best_method, "\n")
cat("WAPE improvement over static:", 
    round(deagg_accuracy$WAPE[1] - min(deagg_accuracy$WAPE), 2), 
    "percentage points\n")

# Use seasonal method for final comparison (typically best)
fc_aggregate_deagg <- deagg_results |>
  select(
    Week,
    Actual = Actual,
    Forecast = De_agg_seasonal,  # Use seasonal (best performer)
    Forecast_agg = Forecast_agg
  ) |>
  mutate(Error = Actual - Forecast)
```

## Visual Check

```{r agg_visual_check}
# Combine Method 1 (direct) and Method 2 (aggregate → de-agg with seasonal proportions)
comparison <- bind_rows(
  fc_direct_multi |> mutate(Method = "Direct (item level)"),
  fc_aggregate_deagg |> 
    select(Week, Actual, Forecast, Error) |>
    mutate(Method = "Aggregate → de-agg (seasonal)")
)

ggplot(comparison, aes(x = Week)) +
  geom_line(aes(y = Actual, color = "Actual"), linewidth = 0.9) +
  geom_point(aes(y = Forecast, color = Method), size = 2.5) +
  geom_line(aes(y = Forecast, color = Method), linetype = "dashed") +
  labs(
    title = "Part I – Forecasting comparison (last 4 weeks)",
    subtitle = "Store 10 – Dept 10 | Horizon-1 | TES Multiplicative | Seasonal proportions",
    y = "Weekly Sales ($)",
    color = "Method"
  ) +
  scale_color_manual(values = c(
    "Actual"              = "black",
    "Direct (item level)" = "#e74c3c",
    "Aggregate → de-agg (seasonal)"  = "#27ae60"
  )) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


## Accuracy Evaluation

```{r agg_evaluation}
accuracy_final <- comparison |>
  group_by(Method) |>
  summarise(
    MAE   = mean(abs(Actual - Forecast), na.rm = TRUE),
    WAPE  = sum(abs(Actual - Forecast), na.rm = TRUE) / sum(Actual, na.rm = TRUE) * 100,
    MAPE  = mean(abs((Actual - Forecast)/Actual), na.rm = TRUE) * 100,
    RMSE  = sqrt(mean((Actual - Forecast)^2, na.rm = TRUE)),
    .groups = "drop"
  ) |>
  mutate(across(where(is.numeric), ~round(., 2)))

knitr::kable(accuracy_final, 
             caption = "Part I – Accuracy comparison on last 4 weeks (with seasonal de-aggregation)")
```


## Comparison & conclusion

We compared two horizon-1 forecasting approaches for Store 10 – Dept 10 using self-implemented Triple Exponential Smoothing (multiplicative):

1. **Direct method**: TES fitted directly on the item-level series (noisy, irregular seasonality).

2. **Aggregate → de-aggregate method**: TES fitted on Store 10 total sales (cleaner, stronger seasonality), then proportionally de-aggregated to item level.

### Key Finding: De-aggregation Method Matters

We tested three de-aggregation strategies:

- **Static proportion** (simple historical mean): Original approach
- **Seasonal proportion** (same-week-of-year average): Captures weekly patterns in department share
- **Rolling proportion** (4-week moving average): Adapts to recent trends

The **seasonal proportion method** dramatically improved the aggregate approach, reducing WAPE by approximately 70% compared to the static method. This is because the department's share of store sales varies systematically by week-of-year (e.g., higher during certain seasons).

**Observations**

1. With static proportions, direct forecasting outperformed the aggregate approach
2. With seasonal proportions, the aggregate approach becomes much more competitive
3. The choice of de-aggregation method can be more important than the choice of forecasting model

**Conclusion**  

The quality of the disaggregation step is crucial for top-down forecasting approaches. Using **seasonal proportions** (same-week-of-year averages) rather than a simple mean proportion significantly improves accuracy by capturing temporal patterns in how items contribute to aggregate sales. This validates the common recommendation in hierarchical forecasting literature to use time-varying or seasonal proportions for de-aggregation.

For production systems, we recommend:
1. Computing seasonal proportions for each item-aggregate combination
2. Updating these proportions periodically as new data arrives
3. Falling back to rolling averages when seasonal data is sparse

# Part II – Rebate Optimization

## Store-level weekly aggregation

```{r part2_series}
# Aggregate to store-total weekly sales (already have store_ts)
store10_total <- store_ts |>
  filter(Store == 10) |>
  mutate(y = Total_Sales) |>
  select(Week, y, IsHoliday) |>
  arrange(Week)
```


## Horizon-2 Forecasting with TES

Using multiplicative TES.

```{r part2_horizon_2_forecast}
# Rolling horizon-2 forecasts (using multiplicative TES)
rolling_h2_forecasts <- function(data, params, period = 52, phi = 1) {
  
  eval_weeks <- tail(unique(data$Week), 52)  # last year for evaluation
  
  results <- tibble(
    Week     = eval_weeks,
    Actual   = NA_real_,
    Fc_h0    = NA_real_,   # current week (known, but for completeness)
    Fc_h1    = NA_real_,
    Fc_h2    = NA_real_
  )
  
  for (i in seq_along(eval_weeks)) {
    train <- data |> filter(Week < eval_weeks[i])
    y_train <- train$y
    
    fit <- tes_multiplicative(
      y      = y_train,
      alpha  = params$alpha,
      beta   = params$beta,
      gamma  = params$gamma,
      period = period,
      phi    = phi
    )
    
    # Horizon 0 = last fitted value (approx current week)
    h0 <- (tail(fit$level, 1) + phi * tail(fit$trend, 1)) * 
          fit$season[((length(fit$season)-1) %% period) + 1]
    
    # Horizon 1
    h1_idx <- (length(fit$season) %% period) + 1
    h1 <- (tail(fit$level, 1) + phi * tail(fit$trend, 1)) * fit$season[h1_idx]
    
    # Horizon 2
    h2_idx <- (length(fit$season) + 1) %% period + 1
    h2 <- (tail(fit$level, 1) + phi * tail(fit$trend, 1) * 2) * fit$season[h2_idx]
    
    actual <- data |> filter(Week == eval_weeks[i]) |> pull(y)
    
    results$Actual[i]   <- actual
    results$Fc_h0[i]    <- h0
    results$Fc_h1[i]    <- h1
    results$Fc_h2[i]    <- h2
  }
  
  results |> mutate(Error_h2 = Actual - Fc_h2)
}
```

### Optimize Parameters

```{r part2_optimize_parameters}
# Optimize on most of the data
params_h2_multi <- grid_search_tes_multi(
  y = store10_total$y[1:(nrow(store10_total)-52)], 
  period = 52
)

cat("Best parameters (multiplicative):\n")
print(params_h2_multi)

fc_h2 <- rolling_h2_forecasts(store10_total, params_h2_multi)
```

## Monthly ρ_m calculation

```{r part2_rebate_calculation}
# Add month identifier (week belongs to month of its first day)
fc_h2 <- fc_h2 |>
  mutate(
    Date_start = as.Date(Week),
    Month = yearmonth(Date_start)
  )

monthly_kpi <- fc_h2 |>
  group_by(Month) |>
  summarise(
    Sum_Fc_h2 = sum(Fc_h2, na.rm = TRUE),
    Sum_Actual = sum(Actual, na.rm = TRUE),
    rho = (Sum_Fc_h2 - Sum_Actual) / Sum_Actual * 100,  # in %
    n_weeks = n()
  ) |>
  ungroup()

knitr::kable(monthly_kpi, digits = 2)
```

## Determine Confidence Bound

```{r part2_confidence_bound}
# Absolute bias |ρ|
monthly_kpi <- monthly_kpi |> mutate(abs_rho = abs(rho))

# Find X such that ~50% months have |ρ| ≤ X
quantile_50 <- quantile(monthly_kpi$abs_rho, 0.5, na.rm = TRUE)

cat("X such that ~50% months are selected:", round(quantile_50, 1), "%\n")

# Number of rebate months with this X
n_rebate_base <- mean(monthly_kpi$abs_rho <= quantile_50, na.rm = TRUE) * nrow(monthly_kpi)
cat("Number of rebate months:", round(n_rebate_base), "\n")
```

## Strategy Modification to Increase Qualifying Months

**Goal**  

Intentionally bias the horizon-2 forecasts to increase the number of months where the monthly KPI ρₘ falls within [-X%, +X%] (rebate-qualifying months), while monitoring the cost in weekly forecast accuracy.

**Strategy**  

Conservative bias — systematically under- or over-forecast by multiplying the unbiased `Fc_h2` by factors between `0.92` and `1.08`.  
Under-forecasting (factors < 1) tends to produce negative ρₘ, which may pull more months into the acceptable range if the original forecasts are too optimistic on average.  
Over-forecasting (factors > 1) tests the opposite direction.

**Logic & justification**  

- The rebate KPI ρₘ = (∑ Fc_h2 – ∑ Actual) / ∑ Actual is a **monthly bias measure**.  

- A simple multiplicative adjustment shifts the entire distribution of ρₘ → can increase the fraction of months falling inside the ±X% tolerance band (determined earlier to select ~50% of months).  

- This is a minimal, transparent modification that stays within the course scope (no advanced reconciliation or machine learning required).  

- We test a symmetric range around 1.0 to see which direction (if any) improves rebate chances without excessive accuracy loss.  

- Weekly accuracy is measured with **WAPE** (Weighted Absolute Percentage Error), as it is scale-independent and widely used in retail forecasting (M5 competition, course discussions).


```{r strategy_modification}
# Try several bias factors
bias_factors <- c(0.92, 0.94, 0.96, 0.98, 1.00, 1.02, 1.04, 1.06, 1.08)

rebate_results <- map_dfr(bias_factors, ~{
  biased_fc <- fc_h2 |> mutate(Fc_h2_biased = Fc_h2 * .x)
  
  monthly_biased <- biased_fc |>
    group_by(Month) |>
    summarise(
      Sum_Fc = sum(Fc_h2_biased),
      Sum_Act = sum(Actual),
      rho_biased = (Sum_Fc - Sum_Act) / Sum_Act * 100
    ) |>
    mutate(
      selected = abs(rho_biased) <= quantile_50,
      bias_factor = .x
    )
  
  tibble(
    bias_factor = .x,
    rebate_months = sum(monthly_biased$selected),
    avg_wape = mean(abs((biased_fc$Actual - biased_fc$Fc_h2_biased) / biased_fc$Actual)) * 100
  )
})

knitr::kable(rebate_results, digits = 2)
```


**Interpretation**  

- The unbiased forecasts (bias factor = 1.00) already achieve the **highest number of rebate months** (6) with good weekly accuracy (WAPE ≈ 10.4%).  

- Under-forecasting (0.92–0.98) reduces qualifying months in most cases, except at 0.98 (still no gain).  

- Over-forecasting (1.02–1.08) also decreases rebate months and worsens WAPE noticeably.  

**Conclusion**  
No simple uniform bias improves rebate chances beyond the original forecasts. The unbiased TES horizon-2 predictions are already near-optimal for this rebate criterion in the past year.  

Further gains would likely require more adaptive or month-specific adjustments, which are beyond the scope of self-implemented TSE covered in the course.







