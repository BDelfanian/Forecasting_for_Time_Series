---
title: "Forecasting Walmart Weekly Sales"
subtitle: "Forecasting for Supply Chain, End-of-Semester Project"
author: "Behrouz Delfanian, Hesam Korki"
date: "28 January 2026"
format: 
  html:
    code-fold: true
    code-overflow: scroll
    code-line-numbers: true
    code-tools: true
    df_print: paged
    toc: true
    toc-location: left
    toc-depth: 3
    smooth-scroll: true
    embed-resources: true
    theme: cosmo
---

# Setup and Environment

## Required Libraries

```{r librairies-to-install}
#| cache: true
#| output: false
cran_repo <- "https://ftp.fau.de/cran"
list_Rpack2use <-
  c("rmarkdown",
    "conflicted",
    "devtools",
    "knitr",
    "quarto",
    "bookdown",
    "DT",
    "Hmisc",
    "tidyverse",
    "magrittr",
    "lubridate",
    "hms",
    "glue",
    "skimr",
    "lobstr",
    "janitor",
    "zeallot",
    "stringi",
    "lemon",
    "crayon",
    "jsonlite",
    "TSA",
    "tsibble",
    "fable",
    "feasts",
    "imputeTS",
    "readxl",
    "slider",
    "reticulate",
    "aws.s3",
    "paws",
    "RAthena",
    "botor",
    "RPostgres",
    "DBI",
    "RJDBC",
    "dbplyr",
    "arrow",
    "geosphere",
    "viridis")

list_installed_packages <- names(installed.packages()[,2])
for (Rpack in list_Rpack2use) {
  if (! Rpack %in% list_installed_packages) {
    print(paste("Installing", Rpack))
    install.packages(Rpack, repos = cran_repo, )
  } else {
    print(paste(Rpack, "already installed."))
  }
}

```

## Load Libraries & Resolve Conflicts

```{r librairies-to-load}
#| cache: false

list_Rpack2load_not <-
  c("conflicted",
    "devtools",
    "paws",
    "RAthena",
    "botor",
    "arrow",
    "RPostgres",
    "DBI",
    "RJDBC",
    "dbplyr",
    "aws.s3",
    "arrow",
    "reticulate",
    "slider",
    "stringi",
    "geosphere",
    "blah")
list_Rpack2load <- setdiff(list_Rpack2use, list_Rpack2load_not)
list_Rpack2load

```

```{r loading-libraries}
#| cache: false
#| output: false
  
for (Rpack in list_Rpack2load) {
  library(Rpack, character.only = TRUE)
}

conflicted::conflicts_prefer(pillar::dim_desc)
conflicted::conflicts_prefer(magrittr::extract)
conflicted::conflicts_prefer(dplyr::filter)
conflicted::conflicts_prefer(jsonlite::flatten)
conflicted::conflicts_prefer(hms::hms)
conflicted::conflicts_prefer(dbplyr::ident)
conflicted::conflicts_prefer(lubridate::interval)
conflicted::conflicts_prefer(dplyr::lag)
conflicted::conflicts_prefer(readxl::read_xlsx)
conflicted::conflicts_prefer(magrittr::set_names)
conflicted::conflicts_prefer(dbplyr::sql)
conflicted::conflicts_prefer(Hmisc::src)
conflicted::conflicts_prefer(dplyr::summarize)
conflicted::conflicts_prefer(magrittr::is_in)

```

# Data Loading & Preparation

## Directory & Data Paths

```{r data_paths}

dir_data <- "./data/Walmart/"

if (!dir.exists(dir_data)) {
  dir.create(dir_data)
}

path_train    <- file.path(dir_data, "train.csv")
path_test     <- file.path(dir_data, "test.csv")
path_features <- file.path(dir_data, "features.csv")
path_stores   <- file.path(dir_data, "stores.csv")

```

## Read Raw Data

```{r read_raw_data}
train_raw <- read_csv(path_train, show_col_types = FALSE)
test_raw  <- read_csv(path_test,  show_col_types = FALSE)
features  <- read_csv(path_features, show_col_types = FALSE)
stores    <- read_csv(path_stores, show_col_types = FALSE)
```

## Merge Data

```{r data_merging}
# Convert dates & create key variables
train <- train_raw |>
  mutate(
    Date = ymd(Date),
    Store_Dept = glue("{Store}-{Dept}"),
    IsHoliday = as.logical(IsHoliday)
  ) |>
  left_join(features |> mutate(Date = ymd(Date)), by = c("Store", "Date", "IsHoliday")) |>
  left_join(stores, by = "Store") |>
  as_tsibble(key = Store_Dept, index = Date)
```

## Check Missing Values

```{r check_missing_values}
na_pct <- colMeans(is.na(train)) * 100
na_pct <- na_pct[na_pct > 0]

data.frame(
  Column = names(na_pct),
  "NA_Percent" = round(na_pct, 2)
) |>
  arrange(desc("NA_Percent"))
```

## Clean Data

**Markdown1** to **Markdown5** columns represent **markdown/promotional discounts** (temporary price reductions) applied in each store on a given week. We replace `NA` with `0` (meaning no markdown/promotion that week). We also remove observations with zero or negative values for `Weekly_Sales`.

```{r clean_data}
train_clean <- train |>
  mutate(
    across(starts_with("MarkDown"), ~ replace_na(., 0))
  )

# Detect problematic sales values
sales_issues <- train_clean |>
  summarise(
    Total_rows          = n(),
    Negative_sales      = sum(Weekly_Sales < 0, na.rm = TRUE),
    Zero_sales          = sum(Weekly_Sales == 0, na.rm = TRUE),
    Negative_or_zero    = sum(Weekly_Sales <= 0, na.rm = TRUE),
    Pct_negative_or_zero = round(mean(Weekly_Sales <= 0, na.rm = TRUE) * 100, 2)
  )

print("Sales quality summary:")
print(sales_issues)

# Remove negative and zero sales (most common and reasonable approach for this project)
train_clean <- train_clean |>
  filter(Weekly_Sales > 0)

# Final check
cat("Rows after removal:", nrow(train_clean), "\n")
cat("Percentage of rows removed:", 
    round((nrow(train) - nrow(train_clean)) / nrow(train) * 100, 2), "%\n")
```

## Train/Test Split

```{r data_split}
all_dates   <- sort(unique(train_clean$Date))
cutoff_date <- all_dates[length(all_dates) - 4]

train_set <- train_clean |> filter(Date <= cutoff_date)
eval_set  <- train_clean |> filter(Date >  cutoff_date)

# Quick verification
cat("Training weeks: ", n_distinct(train_set$Date), "\n")
cat("Evaluation weeks: ", n_distinct(eval_set$Date), "\n")
```

## Different Aggregation Levels

```{r aggregation_levels}
## Create different aggregation levels for Part I comparison

# 1. Item level (Store × Dept) - direct forecasting
item_ts <- train_set |>
  mutate(Week = yearweek(Date)) |>
  as_tsibble(key = c(Store, Dept), index = Week)

# 2. Store level - aggregate then disaggregate
store_ts <- train_set |>
  group_by(Store, Week = yearweek(Date)) |>
  summarise(
    Total_Sales = sum(Weekly_Sales, na.rm = TRUE),
    IsHoliday   = any(IsHoliday),
    .groups     = "drop"
  ) |>
  as_tsibble(key = Store, index = Week)

# 3. Department level
dept_ts <- train_set |>
  group_by(Dept, Week = yearweek(Date)) |>
  summarise(
    Total_Sales = sum(Weekly_Sales, na.rm = TRUE),
    IsHoliday   = any(IsHoliday),
    .groups     = "drop"
  ) |>
  as_tsibble(key = Dept, index = Week)
```

## Quick Overview

```{r quick_overview}
# Number of series
n_series <- n_distinct(train$Store_Dept)
glue("Number of Store-Department time series: {n_series}")

# Time range
range(train$Date)
range(train_set$Date)
range(eval_set$Date)
```

# Time Series Exploratory Analysis

## Seasonality Comparison

To compare the three time series in a normalized way, we apply **Divide-by-Mean** normalization technique.

```{r visual_comparison}
# ── Normalized versions for visual comparison ────────────────────────────────

item_10_10_norm <- item_ts |>
  filter(Store == 10, Dept == 10) |>
  as_tibble() |>
  mutate(
    Sales_norm = Weekly_Sales / mean(Weekly_Sales, na.rm = TRUE),
    Series     = "Store 10 - Dept 10 (item)"
  ) |>
  select(Week, Sales_norm, Series)

store_10_norm <- store_ts |>
  filter(Store == 10) |>
  as_tibble() |>
  mutate(
    Sales_norm = Total_Sales / mean(Total_Sales, na.rm = TRUE),
    Series     = "Store 10 - Total (sum over depts)"
  ) |>
  select(Week, Sales_norm, Series)

dept_10_norm <- dept_ts |>
  filter(Dept == 10) |>
  as_tibble() |>
  mutate(
    Sales_norm = Total_Sales / mean(Total_Sales, na.rm = TRUE),
    Series     = "Dept 10 - Total (sum over stores)"
  ) |>
  select(Week, Sales_norm, Series)

combined_norm <- bind_rows(
  item_10_10_norm,
  store_10_norm,
  dept_10_norm
)

# ── Plot: normalized series (overlay) ────────────────────────────────────────

ggplot(combined_norm, aes(x = Week, y = Sales_norm, color = Series)) +
  geom_line(linewidth = 0.7) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "grey50") +
  scale_color_manual(values = c(
    "Store 10 - Dept 10 (item)"          = "#1f77b4",
    "Store 10 - Total (sum over depts)"  = "#ff7f0e",
    "Dept 10 - Total (sum over stores)"  = "#2ca02c"
  )) +
  labs(
    title    = "Normalized Weekly Sales (divided by own mean)",
    subtitle = "Comparison of shapes: item vs store aggregate vs department aggregate",
    y        = "Normalized sales (mean = 1)",
    x        = "Week",
    color    = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "bottom",
    plot.title      = element_text(face = "bold")
  )

# Faceted version (easier to compare individually)
ggplot(combined_norm, aes(x = Week, y = Sales_norm)) +
  geom_line(color = "#1f77b4", linewidth = 0.7) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "grey50") +
  facet_wrap(~ Series, ncol = 1, scales = "free_y") +
  labs(
    title    = "Normalized Weekly Sales by Series (mean = 1)",
    y        = "Normalized sales",
    x        = NULL
  ) +
  theme_minimal() +
  theme(strip.text = element_text(face = "bold"))
```

We observe much clearer weekly seasonality at the store-total level than at the individual item level.

- **Seasonality strength**: usually strongest at aggregate levels (store total).

- **Noise level**: highest at item level, lowest at aggregates.

- **Holiday effects**: often more visible/clear in aggregates

This suggests that aggregating reduces noise and makes seasonal patterns more detectable → likely benefit for exponential smoothing methods.


## Frequency analysis (periodogram / FFT)


## STL decomposition (trend / seasonal / remainder)


## Additive vs Multiplicative seasonality check


## Stationarity assessment


## Selection of aggregation levels (store / department groups)



# Part I – Forecasting Comparison

We compare two approaches on the last 4 weeks:

1. Direct forecasting at item level (Store 10 – Dept 10)  

2. Forecasting at store level (Store 10 total) → proportional de-aggregation to item level

Both methods use self-implemented Triple Exponential Smoothing (Holt-Winters).

## Self-implemented TSE functions

```{r tse_functions}
# ── Level-only: Simple Exponential Smoothing (SES) ───────────────────────
ses_level <- function(y, alpha, l0 = NULL) {
  n <- length(y)
  l <- numeric(n)
  l[1] <- if (is.null(l0)) y[1] else l0
  for (t in 2:n) {
    l[t] <- alpha * y[t] + (1 - alpha) * l[t-1]
  }
  l
}

ses_forecast1 <- function(l, alpha) {
  l[length(l)]   # horizon 1 forecast = last smoothed level
}

# ── Level + Trend: Double Exponential Smoothing (Holt) ───────────────────
des <- function(y, alpha, beta, l0 = NULL, b0 = NULL, damped = FALSE, phi = 1) {
  n <- length(y)
  l <- b <- numeric(n)
  
  # Initialization (simple heuristic from course)
  l[1] <- if (is.null(l0)) y[1] else l0
  b[1] <- if (is.null(b0)) y[2] - y[1] else b0
  
  for (t in 2:n) {
    l[t] <- alpha * y[t] + (1 - alpha) * (l[t-1] + phi * b[t-1])
    b[t] <- beta  * (l[t] - l[t-1]) + (1 - beta) * phi * b[t-1]
  }
  
  list(level = l, trend = b)
}

des_forecast1 <- function(fit, phi = 1) {
  last_l <- tail(fit$level, 1)
  last_b <- tail(fit$trend, 1)
  last_l + phi * last_b
}

# ── Full TES (Holt-Winters) - additive version ───────────────────────────
tes_additive <- function(y, alpha, beta, gamma, period = 52, 
                         l0 = NULL, b0 = NULL, s0 = NULL, damped = FALSE, phi = 1) {
  n <- length(y)
  if (n < period + 1) stop("Series too short for seasonal model (need > period + 1 points)")
  l <- b <- s <- numeric(n)
  
  # Initialization (course style: average first period for seasonal)
  if (is.null(l0)) l0 <- mean(y[1:period], na.rm = TRUE)
  if (is.null(b0)) b0 <- (mean(y[(period+1):(2*period)]) - mean(y[1:period])) / period
  if (is.null(s0)) s0 <- y[1:period] - l0
  
  l[1] <- l0
  b[1] <- b0
  s[1:period] <- s0
  
  for (t in (period + 1):n) {
    l[t] <- alpha * (y[t] - s[t-period]) + (1 - alpha) * (l[t-1] + phi * b[t-1])
    b[t] <- beta  * (l[t] - l[t-1]) + (1 - beta) * phi * b[t-1]
    s[t] <- gamma * (y[t] - l[t]) + (1 - gamma) * s[t-period]
  }
  
  list(level = l, trend = b, season = s)
}

tes_add_forecast1 <- function(fit, period = 52, phi = 1) {
  last_l <- tail(fit$level, 1)
  last_b <- tail(fit$trend, 1)
  m <- length(fit$season)
  next_season_idx <- ((m - 1) %% period) + 1
  next_s <- fit$season[next_season_idx]
  last_l + phi * last_b + next_s
}

# ── TES Multiplicative version ───────────────────────────────────────────

tes_multiplicative <- function(y, alpha, beta, gamma, period = 52,
                               l0 = NULL, b0 = NULL, s0 = NULL,
                               damped = FALSE, phi = 1) {
  n <- length(y)
  if (n < period + 1) stop("Series too short for seasonal model (need > period + 1 points)")
  l <- b <- s <- numeric(n)
  
  # Initialization (common heuristic)
  if (is.null(l0)) l0 <- mean(y[1:period], na.rm = TRUE)
  if (any(s0 <= 0)) {
  warning("Zero or negative seasonal components detected – replacing with small value")
  s0[s0 <= 0] <- 1e-6
  }
  if (is.null(b0)) b0 <- (mean(y[(period+1):(2*period)]) - mean(y[1:period])) / period
  if (is.null(s0)) s0 <- y[1:period] / l0                     # ← key difference
  
  l[1] <- l0
  b[1] <- b0
  s[1:period] <- s0
  
  for (t in (period + 1):n) {
    l[t] <- alpha * (y[t] / s[t - period]) + (1 - alpha) * (l[t-1] + phi * b[t-1])
    b[t] <- beta  * (l[t] - l[t-1])            + (1 - beta)  * phi * b[t-1]
    s[t] <- gamma * (y[t] / l[t])              + (1 - gamma) * s[t - period]
  }
  
  list(level = l, trend = b, season = s)
}

# One-step forecast
tes_multi_forecast1 <- function(fit, period = 52, phi = 1) {
  last_l <- tail(fit$level,  1)
  last_b <- tail(fit$trend,  1)
  m <- length(fit$season)
  next_season_idx <- ((m - 1) %% period) + 1
  next_s <- fit$season[next_season_idx]
  
  (last_l + phi * last_b) * next_s
}

# SSE objective for TES additive
sse_tes_add <- function(params, y, period = 52, damped = FALSE, phi = 1) {
  alpha <- params[1]; beta <- params[2]; gamma <- params[3]
  if (any(c(alpha, beta, gamma) <= 0) || any(c(alpha, beta, gamma) >= 1)) return(Inf)

  fit <- tes_additive(y, alpha, beta, gamma, period, damped = damped, phi = phi)
  errors <- y - (fit$level + fit$trend * phi + fit$season)   # in-sample one-step errors
  sum(errors^2, na.rm = TRUE)
}

sse_tes_multi <- function(params, y, period = 52, phi = 1) {
  alpha <- params[1]; beta <- params[2]; gamma <- params[3]
  if (any(c(alpha, beta, gamma) <= 0) || any(c(alpha, beta, gamma) >= 1)) return(Inf)
  
  fit <- tes_multiplicative(y, alpha, beta, gamma, period, phi = phi)
  
  # One-step fitted values
  fitted <- numeric(length(y))
  for (t in 1:length(y)) {
    s_idx <- ((t - 1) %% period) + 1
    fitted[t] <- (fit$level[t] + phi * fit$trend[t]) * fit$season[s_idx]
  }
  
  errors <- y - fitted
  sum(errors^2, na.rm = TRUE)
}

# Grid search example (coarse)
grid_search_tes_add <- function(y, period = 52) {
  cat("Starting grid search (additive)...\n")
  
  alphas <- seq(0.1, 0.9, 0.1)
  betas  <- seq(0.01, 0.5, 0.05)
  gammas <- seq(0.1, 0.8, 0.1)
  
  best_sse <- Inf
  best_par <- c(0.5, 0.1, 0.3)
  
  for (a in alphas) for (b in betas) for (g in gammas) {
    sse <- sse_tes_add(c(a,b,g), y, period)
    if (sse < best_sse) {
      best_sse <- sse
      best_par <- c(a,b,g)
    }
  }
  list(alpha = best_par[1], beta = best_par[2], gamma = best_par[3], sse = best_sse)
}

grid_search_tes_multi <- function(y, period = 52) {
  cat("Starting grid search (multiplicative)...\n")
  
  alphas <- seq(0.1, 0.9, 0.1)
  betas  <- seq(0.01, 0.5, 0.05)
  gammas <- seq(0.1, 0.8, 0.1)
  
  best_sse <- Inf
  best_par <- c(0.5, 0.1, 0.3)
  
  for (a in alphas) for (b in betas) for (g in gammas) {
    sse <- sse_tes_multi(c(a,b,g), y, period)
    if (sse < best_sse) {
      best_sse <- sse
      best_par <- c(a,b,g)
    }
  }
  list(alpha = best_par[1], beta = best_par[2], gamma = best_par[3], sse = best_sse)
}

# Rolling horizon-1 forecasts – generic for any fitted TSE model
rolling_h1_forecasts <- function(data, y_var = "y", period = 52,
                                 params, model = "multi", phi = 1) {
  
  eval_weeks <- tail(unique(data$Week), 4)
  results <- tibble(Week = eval_weeks, Actual = NA_real_, Forecast = NA_real_)
  
  for (i in seq_along(eval_weeks)) {
    train <- data |> filter(Week < eval_weeks[i])
    y_train <- train[[y_var]]
    
    
    if (model == "add") {
      fit <- tes_additive(y_train, params$alpha, params$beta, params$gamma,
                          period = period, phi = phi)
      f1 <- tes_add_forecast1(fit, period = period, phi = phi)
    } else {
      fit <- tes_multiplicative(y_train, params$alpha, params$beta, params$gamma,
                                period = period, phi = phi)
      f1 <- tes_multi_forecast1(fit, period = period, phi = phi)
    }
    
    actual <- data |> filter(Week == eval_weeks[i]) |> pull({{y_var}})
    
    results$Actual[i]   <- actual
    results$Forecast[i] <- f1
  }
  
  results |> mutate(Error = Actual - Forecast)
}
```


## Method 1: Direct forecasting at Store-Dept level

```{r item_series}
# ── Prepare the series for Store 10 – Dept 10 ────────────────────────────────
item_series <- item_ts |>
  filter(Store == 10, Dept == 10) |>
  mutate(y = Weekly_Sales) |>
  select(Week, y, IsHoliday) |>
  as_tibble()   # easier for fitting

# Quick check
glimpse(item_series)
```

### Parameter Optimization

Optimize parameters using multiplicative model.

```{r parameter_optimization_multi}
params_item_multi <- grid_search_tes_multi(item_series$y, period = 52)

# Show results
cat("Best parameters (multiplicative) for Store 10 – Dept 10:\n")
print(params_item_multi)
```
- **α = 0.9** (level smoothing parameter)  
Very high value → the model puts **strong weight on the most recent observation** when updating the level (smoothed baseline).  
→ The level adapts **very quickly** to recent changes in the series.  

**β = 0.01** (trend smoothing parameter)  
Extremely low value → the trend component is updated **very slowly** / almost not at all.  
→ The model essentially treats the trend as **nearly constant** over time.  
→ This is common when there is **little or no clear long-term trend** in the data, or when trend changes are very gradual.

**γ = 0.1** (seasonal smoothing parameter)  
Moderate-to-low value → the seasonal pattern is updated **slowly**.  
→ The model trusts the **historical seasonal pattern quite strongly** and only makes small adjustments when new data arrives.  
→ Suitable when seasonality is **stable over time** (which is often the case at aggregate levels in retail data).

**Overall interpretation in context**  
- The model is behaving almost like **simple exponential smoothing with a fixed seasonal pattern** (because β is extremely small and γ is low).  
- The very high α (0.9) indicates that the series is **noisy or has fast level shifts**, so the model reacts strongly to recent values.  


In short:

> The optimized parameters (α = 0.9, β = 0.01, γ = 0.1) indicate that the model relies heavily on recent observations to update the level (high α), while treating trend as almost constant (very low β) and preserving a stable seasonal pattern (low γ). This is consistent with the noisy and irregular behavior observed at the individual item level.

Also additive model for comparison:

```{r parameter_optimization_add}
params_item_add <- grid_search_tes_add(item_series$y, period = 52)
cat("\nBest parameters (additive) for Store 10 – Dept 10:\n")
print(params_item_add)
```


### Rolling Horizon-1 Forecasts

```{r horizon_1_forecast}
# Rolling forecasts – multiplicative version
fc_direct_multi <- rolling_h1_forecasts(
  data   = item_series,
  y_var  = "y",
  period = 52,
  params = params_item_multi,
  model  = "multi",
  phi    = 1
)

# Additive version
fc_direct_add <- rolling_h1_forecasts(
  data   = item_series,
  y_var  = "y",
  period = 52,
  params = params_item_add,
  model  = "add",
  phi    = 1
)

# Combine for comparison
fc_compare <- bind_rows(
  fc_direct_multi |> mutate(Model = "Multiplicative"),
  fc_direct_add   |> mutate(Model = "Additive")
)
```

### Visual Check

```{r visual_check}
# Plot comparison
ggplot(fc_compare, aes(x = Week)) +
  geom_line(aes(y = Actual, color = "Actual"), linewidth = 0.9) +
  geom_point(aes(y = Forecast, color = Model), size = 2.5) +
  geom_line(aes(y = Forecast, color = Model), linetype = "dashed") +
  scale_color_manual(values = c("Actual" = "black", "Multiplicative" = "red", "Additive" = "blue")) +
  labs(
    title = "Direct forecasting – Additive vs Multiplicative",
    subtitle = "Store 10 – Dept 10 | Last 4 weeks",
    y = "Weekly Sales"
  ) +
  theme_minimal()
```

### Accuracy Evaluation

We evaluate forecast accuracy using the following four measures:

- **MAE** (Mean Absolute Error)  
  Average absolute difference between forecast and actual.  
  Easy to interpret in the original units ($), robust to outliers compared to squared errors.

- **WAPE** (Weighted Absolute Percentage Error) = Σ |error| / Σ actual × 100  
  Scale-independent version of absolute error — expresses total error relative to total demand.  
  Preferred in retail forecasting when aggregating over items or periods (avoids division by small/zero actuals).

- **MAPE** (Mean Absolute Percentage Error) = mean(|error / actual| × 100)  
  Classic percentage error measure — shows average relative error per observation.  
  Intuitive, but sensitive when actual values are close to zero (which we mitigated by removing zero sales earlier).

- **RMSE** (Root Mean Squared Error)  
  Square root of average squared errors — penalizes large errors more heavily.  
  Useful when large deviations are particularly costly (common in inventory/supply chain contexts).

These metrics are standard in forecasting literature and were covered/discussed in the course materials (especially in sessions on model evaluation and confidence intervals).  

They provide complementary views:  

- MAE & RMSE → absolute scale & outlier sensitivity  

- MAPE & WAPE → relative performance & scale-invariance

We report all four to give a complete picture, with **WAPE** receiving particular attention because it is widely used in retail demand forecasting (including the M5 competition from which this dataset originates).

```{r accuracy_comparison}
# Accuracy comparison table
accuracy_compare <- fc_compare |>
  group_by(Model) |>
  summarise(
    MAE  = mean(abs(Error), na.rm = TRUE),
    WAPE = sum(abs(Error), na.rm = TRUE) / sum(Actual, na.rm = TRUE) * 100,
    MAPE = mean(abs(Error / Actual), na.rm = TRUE) * 100,
    RMSE = sqrt(mean(Error^2, na.rm = TRUE))
  ) |>
  mutate(across(where(is.numeric), ~round(., 2)))

knitr::kable(accuracy_compare, caption = "Method 1 – Additive vs Multiplicative")
```


## Method 2: Aggregate → Forecast → Proportional de-aggregation

Preparing aggregate series (Store 10 total sales).

```{r aggregated_series}
store_series <- store_ts |>
  filter(Store == 10) |>
  mutate(y = Total_Sales) |>
  select(Week, y, IsHoliday) |>
  as_tibble()

cat("Store 10 total series length:", nrow(store_series), "weeks\n")
```

## Parameter Optimization

Multiplicative (stronger seasonality at aggregate level):

```{r agg_para_opti_multi}
params_store_multi <- grid_search_tes_multi(store_series$y, period = 52)

cat("Best parameters (multiplicative) for Store 10 total:\n")
print(params_store_multi)
```
Additive for comparison:

```{r agg_para_opti_add}
params_store_add <- grid_search_tes_add(store_series$y, period = 52)
cat("\nBest parameters (additive):\n")
print(params_store_add)
```

## Rolling Horizon-1 Forecast

Using multiplicative model:

```{r agg_horizon_1_forecast_multi}
fc_aggregate_multi <- rolling_h1_forecasts(
  data   = store_series,
  y_var  = "y",
  period = 52,
  params = params_store_multi,
  model  = "multi",
  phi    = 1
)
```


## Proportional De-aggregation

To obtain item-level forecasts from the store-level forecast, we apply **proportional de-aggregation** using the historical average share of Dept 10 in the total sales of Store 10.

**Why this method?**

- It is the simplest and most transparent way to distribute the aggregate forecast back to the item level.

- The project instructions explicitly require forecasting at an aggregate level and **then de-aggregating to the item level**, but do not prescribe a specific technique. No advanced reconciliation methods (bottom-up, top-down, MinTrace, etc.) were covered in the course materials.

- The historical proportion assumes that the relative contribution of Dept 10 to the store total is reasonably stable over time — a reasonable first approximation given the course focus on basic exponential smoothing techniques.

- This approach is widely used in practice when lower-level series are noisy and the aggregate series shows clearer patterns (as observed in the normalized seasonality plot earlier).

- Alternative simple methods (e.g. uniform split by number of departments) would be less accurate, as departments have very different sales volumes.

**Implementation**

- Compute the average share:  
  `prop = mean(Dept 10 sales / Store 10 total sales)` over all training weeks  
- Apply it to each horizon-1 aggregate forecast:  
  `Forecast_item = Forecast_store × prop`

This method ensures that the de-aggregated forecasts remain **consistent** with the aggregate forecast (i.e., if summed over all departments, they would recover the store-level forecast).

**Limitations**

The proportion is static and does not adapt to recent changes in department mix.

```{r de_aggregation}
prop_dept10 <- item_series |>
  left_join(store_series |> select(Week, Total = y), by = "Week") |>
  summarise(prop = mean(y / Total, na.rm = TRUE)) |>
  pull(prop)

cat("Historical average share of Dept 10 in Store 10 total:", round(prop_dept10, 4), "\n")

# Apply proportion to aggregate forecasts
fc_aggregate_deagg <- fc_aggregate_multi |>
  mutate(
    Forecast_item = Forecast * prop_dept10,
    Actual_item   = item_series |> 
      filter(Week %in% fc_aggregate_multi$Week) |> 
      pull(y)
  ) |>
  rename(Forecast_aggregate = Forecast) |>
  select(Week, Actual_item, Forecast_item, everything())

# Rename for clarity
fc_aggregate_deagg <- fc_aggregate_deagg |>
  select(
    Week,
    Actual   = Actual_item,
    Forecast = Forecast_item,
    Forecast_agg = Forecast_aggregate,
    Actual_agg   = Actual,
    Error_agg    = Error
  )
```

## Visual Check

```{r agg_visual_check}
# Combine Method 1 (direct) and Method 2 (aggregate → de-agg)
comparison <- bind_rows(
  fc_direct_multi |> mutate(Method = "Direct (item level)"),
  fc_aggregate_deagg |> mutate(Method = "Aggregate → de-agg")
)

ggplot(comparison, aes(x = Week)) +
  geom_line(aes(y = Actual, color = "Actual"), linewidth = 0.9) +
  geom_point(aes(y = Forecast, color = Method), size = 2.5) +
  geom_line(aes(y = Forecast, color = Method), linetype = "dashed") +
  labs(
    title = "Part I – Forecasting comparison (last 4 weeks)",
    subtitle = "Store 10 – Dept 10 | Horizon-1 | TES Multiplicative",
    y = "Weekly Sales ($)",
    color = "Method"
  ) +
  scale_color_manual(values = c(
    "Actual"              = "black",
    "Direct (item level)" = "#e74c3c",
    "Aggregate → de-agg"  = "#27ae60"
  )) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


## Accuracy Evaluation

```{r agg_evaluation}
accuracy_final <- comparison |>
  group_by(Method) |>
  summarise(
    MAE   = mean(abs(Actual - Forecast), na.rm = TRUE),
    WAPE  = sum(abs(Actual - Forecast), na.rm = TRUE) / sum(Actual, na.rm = TRUE) * 100,
    MAPE  = mean(abs((Actual - Forecast)/Actual), na.rm = TRUE) * 100,
    RMSE  = sqrt(mean((Actual - Forecast)^2, na.rm = TRUE)),
    .groups = "drop"
  ) |>
  mutate(across(where(is.numeric), ~round(., 2)))

knitr::kable(accuracy_final, 
             caption = "Part I – Accuracy comparison on last 4 weeks")
```


## Comparison & conclusion

We compared two horizon-1 forecasting approaches for Store 10 – Dept 10 using self-implemented Triple Exponential Smoothing (multiplicative):

1. **Direct method**: TES fitted directly on the item-level series (noisy, irregular seasonality).

2. **Aggregate → de-aggregate method**: TES fitted on Store 10 total sales (cleaner, stronger seasonality), then proportionally de-aggregated using the historical average share of Dept 10.

**Observation**  
Contrary to the typical expectation, the aggregate-then-deaggregate approach did **not** improve accuracy compared to direct item-level forecasting in this specific case.

**Possible reasons**  
- The chosen item (Store 10 – Dept 10) may have department-specific patterns or promotions that are **not well captured** by the simple average proportion from the whole store.  
- The noise level at item level, while high, may still contain useful local signals that are lost when aggregating.  
- The historical proportion used for de-aggregation is static and may not adapt well to recent changes in department contribution.  
- Parameter optimization and model choice (multiplicative) worked reasonably on both levels, but the short evaluation period (only 4 weeks) makes results sensitive to individual holiday or promotion effects.

**Conclusion**  
In this particular Store–Dept combination, direct forecasting at the item level performed better (lower WAPE/MAE). This highlights that aggregation benefits are **not automatic** — they depend on how representative the aggregate series is of the target item and on the quality of the disaggregation step. For more robust conclusions, one could test multiple items, use dynamic proportions, or apply bottom-up/top-down reconciliation methods (beyond the scope of this project).

# Part II – Rebate Optimization

## Store-level weekly aggregation

```{r part2_series}
# Aggregate to store-total weekly sales (already have store_ts)
store10_total <- store_ts |>
  filter(Store == 10) |>
  mutate(y = Total_Sales) |>
  select(Week, y, IsHoliday) |>
  arrange(Week)
```


## Horizon-2 Forecasting with TES

Using multiplicative TES.

```{r part2_horizon_2_forecast}
# Rolling horizon-2 forecasts (using multiplicative TES)
rolling_h2_forecasts <- function(data, params, period = 52, phi = 1) {
  
  eval_weeks <- tail(unique(data$Week), 52)  # last year for evaluation
  
  results <- tibble(
    Week     = eval_weeks,
    Actual   = NA_real_,
    Fc_h0    = NA_real_,   # current week (known, but for completeness)
    Fc_h1    = NA_real_,
    Fc_h2    = NA_real_
  )
  
  for (i in seq_along(eval_weeks)) {
    train <- data |> filter(Week < eval_weeks[i])
    y_train <- train$y
    
    fit <- tes_multiplicative(
      y      = y_train,
      alpha  = params$alpha,
      beta   = params$beta,
      gamma  = params$gamma,
      period = period,
      phi    = phi
    )
    
    # Horizon 0 = last fitted value (approx current week)
    h0 <- (tail(fit$level, 1) + phi * tail(fit$trend, 1)) * 
          fit$season[((length(fit$season)-1) %% period) + 1]
    
    # Horizon 1
    h1_idx <- (length(fit$season) %% period) + 1
    h1 <- (tail(fit$level, 1) + phi * tail(fit$trend, 1)) * fit$season[h1_idx]
    
    # Horizon 2
    h2_idx <- (length(fit$season) + 1) %% period + 1
    h2 <- (tail(fit$level, 1) + phi * tail(fit$trend, 1) * 2) * fit$season[h2_idx]
    
    actual <- data |> filter(Week == eval_weeks[i]) |> pull(y)
    
    results$Actual[i]   <- actual
    results$Fc_h0[i]    <- h0
    results$Fc_h1[i]    <- h1
    results$Fc_h2[i]    <- h2
  }
  
  results |> mutate(Error_h2 = Actual - Fc_h2)
}
```

### Optimize Parameters

```{r part2_optimize_parameters}
# Optimize on most of the data
params_h2_multi <- grid_search_tes_multi(
  y = store10_total$y[1:(nrow(store10_total)-52)], 
  period = 52
)

cat("Best parameters (multiplicative):\n")
print(params_h2_multi)

fc_h2 <- rolling_h2_forecasts(store10_total, params_h2_multi)
```

## Monthly ρ_m calculation

```{r part2_rebate_calculation}
# Add month identifier (week belongs to month of its first day)
fc_h2 <- fc_h2 |>
  mutate(
    Date_start = as.Date(Week),
    Month = yearmonth(Date_start)
  )

monthly_kpi <- fc_h2 |>
  group_by(Month) |>
  summarise(
    Sum_Fc_h2 = sum(Fc_h2, na.rm = TRUE),
    Sum_Actual = sum(Actual, na.rm = TRUE),
    rho = (Sum_Fc_h2 - Sum_Actual) / Sum_Actual * 100,  # in %
    n_weeks = n()
  ) |>
  ungroup()

knitr::kable(monthly_kpi, digits = 2)
```

## Determine Confidence Bound

```{r part2_confidence_bound}
# Absolute bias |ρ|
monthly_kpi <- monthly_kpi |> mutate(abs_rho = abs(rho))

# Find X such that ~50% months have |ρ| ≤ X
quantile_50 <- quantile(monthly_kpi$abs_rho, 0.5, na.rm = TRUE)

cat("X such that ~50% months are selected:", round(quantile_50, 1), "%\n")

# Number of rebate months with this X
n_rebate_base <- mean(monthly_kpi$abs_rho <= quantile_50, na.rm = TRUE) * nrow(monthly_kpi)
cat("Number of rebate months:", round(n_rebate_base), "\n")
```

## Strategy Modification to Increase Qualifying Months

**Goal**  

Intentionally bias the horizon-2 forecasts to increase the number of months where the monthly KPI ρₘ falls within [-X%, +X%] (rebate-qualifying months), while monitoring the cost in weekly forecast accuracy.

**Strategy**  

Conservative bias — systematically under- or over-forecast by multiplying the unbiased `Fc_h2` by factors between `0.92` and `1.08`.  
Under-forecasting (factors < 1) tends to produce negative ρₘ, which may pull more months into the acceptable range if the original forecasts are too optimistic on average.  
Over-forecasting (factors > 1) tests the opposite direction.

**Logic & justification**  

- The rebate KPI ρₘ = (∑ Fc_h2 – ∑ Actual) / ∑ Actual is a **monthly bias measure**.  

- A simple multiplicative adjustment shifts the entire distribution of ρₘ → can increase the fraction of months falling inside the ±X% tolerance band (determined earlier to select ~50% of months).  

- This is a minimal, transparent modification that stays within the course scope (no advanced reconciliation or machine learning required).  

- We test a symmetric range around 1.0 to see which direction (if any) improves rebate chances without excessive accuracy loss.  

- Weekly accuracy is measured with **WAPE** (Weighted Absolute Percentage Error), as it is scale-independent and widely used in retail forecasting (M5 competition, course discussions).


```{r strategy_modification}
# Try several bias factors
bias_factors <- c(0.92, 0.94, 0.96, 0.98, 1.00, 1.02, 1.04, 1.06, 1.08)

rebate_results <- map_dfr(bias_factors, ~{
  biased_fc <- fc_h2 |> mutate(Fc_h2_biased = Fc_h2 * .x)
  
  monthly_biased <- biased_fc |>
    group_by(Month) |>
    summarise(
      Sum_Fc = sum(Fc_h2_biased),
      Sum_Act = sum(Actual),
      rho_biased = (Sum_Fc - Sum_Act) / Sum_Act * 100
    ) |>
    mutate(
      selected = abs(rho_biased) <= quantile_50,
      bias_factor = .x
    )
  
  tibble(
    bias_factor = .x,
    rebate_months = sum(monthly_biased$selected),
    avg_wape = mean(abs((biased_fc$Actual - biased_fc$Fc_h2_biased) / biased_fc$Actual)) * 100
  )
})

knitr::kable(rebate_results, digits = 2)
```


**Interpretation**  

- The unbiased forecasts (bias factor = 1.00) already achieve the **highest number of rebate months** (6) with good weekly accuracy (WAPE ≈ 10.4%).  

- Under-forecasting (0.92–0.98) reduces qualifying months in most cases, except at 0.98 (still no gain).  

- Over-forecasting (1.02–1.08) also decreases rebate months and worsens WAPE noticeably.  

**Conclusion**  
No simple uniform bias improves rebate chances beyond the original forecasts. The unbiased TES horizon-2 predictions are already near-optimal for this rebate criterion in the past year.  

Further gains would likely require more adaptive or month-specific adjustments, which are beyond the scope of self-implemented TSE covered in the course.







